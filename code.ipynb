{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b02b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame head:\n",
      "   id  age          job  marital  education default  balance housing loan  \\\n",
      "0   0   42   technician  married  secondary      no        7      no   no   \n",
      "1   1   38  blue-collar  married  secondary      no      514      no   no   \n",
      "2   2   36  blue-collar  married  secondary      no      602     yes   no   \n",
      "3   3   27      student   single  secondary      no       34     yes   no   \n",
      "4   4   26   technician  married  secondary      no      889     yes   no   \n",
      "\n",
      "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
      "0  cellular   25   aug       117         3     -1         0  unknown  0  \n",
      "1   unknown   18   jun       185         1     -1         0  unknown  0  \n",
      "2   unknown   14   may       111         2     -1         0  unknown  0  \n",
      "3   unknown   28   may        10         2     -1         0  unknown  0  \n",
      "4  cellular    3   feb       902         1     -1         0  unknown  1  \n",
      "\n",
      "Test DataFrame head:\n",
      "       id  age            job  marital  education default  balance housing  \\\n",
      "0  750000   32    blue-collar  married  secondary      no     1397     yes   \n",
      "1  750001   44     management  married   tertiary      no       23     yes   \n",
      "2  750002   36  self-employed  married    primary      no       46     yes   \n",
      "3  750003   58    blue-collar  married  secondary      no    -1380     yes   \n",
      "4  750004   28     technician   single  secondary      no     1950     yes   \n",
      "\n",
      "  loan   contact  day month  duration  campaign  pdays  previous poutcome  \n",
      "0   no   unknown   21   may       224         1     -1         0  unknown  \n",
      "1   no  cellular    3   apr       586         2     -1         0  unknown  \n",
      "2  yes  cellular   13   may       111         2     -1         0  unknown  \n",
      "3  yes   unknown   29   may       125         1     -1         0  unknown  \n",
      "4   no  cellular   22   jul       181         1     -1         0  unknown  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths using raw strings\n",
    "train_file_path = r'D:\\manga\\MACHINE,DEEP LEARNING\\binary classification project\\train.csv'\n",
    "test_file_path = r'D:\\manga\\MACHINE,DEEP LEARNING\\binary classification project\\test.csv'\n",
    "\n",
    "# Load the CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "# Display the first 5 rows of each DataFrame to confirm they loaded correctly\n",
    "print(\"Train DataFrame head:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame head:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522e056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750000 entries, 0 to 749999\n",
      "Data columns (total 18 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   id         750000 non-null  int64 \n",
      " 1   age        750000 non-null  int64 \n",
      " 2   job        750000 non-null  object\n",
      " 3   marital    750000 non-null  object\n",
      " 4   education  750000 non-null  object\n",
      " 5   default    750000 non-null  object\n",
      " 6   balance    750000 non-null  int64 \n",
      " 7   housing    750000 non-null  object\n",
      " 8   loan       750000 non-null  object\n",
      " 9   contact    750000 non-null  object\n",
      " 10  day        750000 non-null  int64 \n",
      " 11  month      750000 non-null  object\n",
      " 12  duration   750000 non-null  int64 \n",
      " 13  campaign   750000 non-null  int64 \n",
      " 14  pdays      750000 non-null  int64 \n",
      " 15  previous   750000 non-null  int64 \n",
      " 16  poutcome   750000 non-null  object\n",
      " 17  y          750000 non-null  int64 \n",
      "dtypes: int64(9), object(9)\n",
      "memory usage: 103.0+ MB\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250000 entries, 0 to 249999\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   id         250000 non-null  int64 \n",
      " 1   age        250000 non-null  int64 \n",
      " 2   job        250000 non-null  object\n",
      " 3   marital    250000 non-null  object\n",
      " 4   education  250000 non-null  object\n",
      " 5   default    250000 non-null  object\n",
      " 6   balance    250000 non-null  int64 \n",
      " 7   housing    250000 non-null  object\n",
      " 8   loan       250000 non-null  object\n",
      " 9   contact    250000 non-null  object\n",
      " 10  day        250000 non-null  int64 \n",
      " 11  month      250000 non-null  object\n",
      " 12  duration   250000 non-null  int64 \n",
      " 13  campaign   250000 non-null  int64 \n",
      " 14  pdays      250000 non-null  int64 \n",
      " 15  previous   250000 non-null  int64 \n",
      " 16  poutcome   250000 non-null  object\n",
      "dtypes: int64(8), object(9)\n",
      "memory usage: 32.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check the training data\n",
    "print(\"Training Data Info:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Check the test data\n",
    "print(\"Test Data Info:\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae21257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution (Counts):\n",
      "y\n",
      "0    659512\n",
      "1     90488\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target variable distribution (Percentages):\n",
      "y\n",
      "0    87.934933\n",
      "1    12.065067\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the counts of each class in the target variable\n",
    "print(\"Target variable distribution (Counts):\")\n",
    "print(train_df['y'].value_counts())\n",
    "\n",
    "print(\"\\nTarget variable distribution (Percentages):\")\n",
    "print(train_df['y'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811d0c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightgbm) (1.13.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 660.6 kB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.0/1.5 MB 393.8 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.1/1.5 MB 939.4 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.2/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.4/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327fb08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of preprocessed training data: (750000, 52)\n",
      "Shape of preprocessed test data: (250000, 52)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Load the data again (if not already in memory)\n",
    "train_df = pd.read_csv(r'D:\\manga\\MACHINE,DEEP LEARNING\\binary classification project\\train.csv')\n",
    "test_df = pd.read_csv(r'D:\\manga\\MACHINE,DEEP LEARNING\\binary classification project\\test.csv')\n",
    "\n",
    "# Separate features (X) from the target (y)\n",
    "X = train_df.drop('y', axis=1)\n",
    "y = train_df['y']\n",
    "\n",
    "# Separate numerical and categorical columns automatically\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create the preprocessor using ColumnTransformer\n",
    "# It applies OneHotEncoder to categorical features and leaves numerical ones alone\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Apply the preprocessing to both training and test data\n",
    "# We fit the preprocessor on the training data only\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "# We transform the test data using the fitted preprocessor from the training data\n",
    "X_test_encoded = preprocessor.transform(test_df)\n",
    "\n",
    "print(\"Shape of preprocessed training data:\", X_encoded.shape)\n",
    "print(\"Shape of preprocessed test data:\", X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d77277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131869 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 52\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "Model training completed successfully! 🎉\n"
     ]
    }
   ],
   "source": [
    "# Split the preprocessed training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate the LightGBM classifier with class_weight='balanced'\n",
    "model = LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel training completed successfully! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a236786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC Score on the validation set is: 0.96484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "# We need the probability of the positive class (1), which is the second column [:, 1]\n",
    "val_probabilities = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "auc_score = roc_auc_score(y_val, val_probabilities)\n",
    "\n",
    "print(f\"The ROC AUC Score on the validation set is: {auc_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0ab1432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved successfully to: D:\\manga\\MACHINE,DEEP LEARNING\\binary classification project\\submission.csv ✅\n"
     ]
    }
   ],
   "source": [
    "# Define the full path for the submission file using a raw string\n",
    "output_path = r'D:\\manga\\MACHINE,DEEP LEARNING\\binary classification project\\submission.csv'\n",
    "\n",
    "# Predict probabilities on the preprocessed test data\n",
    "test_probabilities = model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# Create a DataFrame for the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'y': test_probabilities})\n",
    "\n",
    "# Save the DataFrame to the specified path\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Submission file saved successfully to: {output_path} ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2937458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
